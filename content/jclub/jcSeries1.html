---
title: "Series 1: Bad science"
author: Giusi Moffa
date: 2017-06-01
image: "jclub/img/badScience.jpeg"
description: A light start into evidence based medicine
draft: false
type: "post"
---



<!-- Dates of first reading series -->
<p>For a light start in the field of evidence based science Goldacre’s <a href="http://www.badscience.net/"><code>Bad science</code></a> is a good and enjoyable read.</p>
<p><a href="https://en.wikipedia.org/wiki/Bad_Science_(book)">Bad science wiki page</a></p>
<p><a href="https://www.amazon.de/Bad-Science-Ben-Goldacre/dp/000728487X/ref=sr_1_1?ie=UTF8&amp;qid=1493295393&amp;sr=8-1&amp;keywords=Bad+Science">Order Bad science from amazon</a></p>
<p>Goldacre is a British physician, academic and Guardian columnist, very enjoyable to read, though provocative.</p>
<p>An interesting follow up is Goldacre’s <a href="https://www.wikiwand.com/en/Bad_Pharma"><code>Bad Pharma</code></a>, more closely oriented to the world of drug development. Unsurprisingly it has attracted some criticism by reknown statisticians working in this field, such as Stephen Senn’s:</p>
<p><a href="https://www.ucl.ac.uk/statistics/biostatistics-network/Seminars_Talks/StephenSenn">Bad JAMA - slides</a><br />
<a href="http://www.phil.vt.edu/dmayo/personal_website/SENN-Bad%20JAMA%20V7.pdf">Bad JAMA - article</a><br />
<a href="https://errorstatistics.com/2013/01/06/guest-post-bad-pharma-s-senn/">Bad JAMA/Pharma on the blog</a><br />
Especially highlighting the bias which may be associated with (possibly unkown) filtering of the data before analysis, in the sense that the sample used for analysis is not a simple random sample, especially in relation to assess editorial bias against negative studies. Finding that the probability of acceptance of submitted studies is the same regardless of whether the study results are positive or negative is the same, does not imply no bias, since it does not account for the fact that negative studies submitted might be of higher quality on average, since authors are aware of the editorial pre-disposition against negative studies unless they are of outstanding quality.<br />
<a href="http://www.senns.demon.co.uk/Bad%20Karma%20pre%20publication.pdf">Bad karma -article</a><br />
<a href="https://errorstatistics.com/2013/03/07/stephen-senn-casting-stones/">Casting Stones, by Stephen Senn</a><br />
A piece about the strategies to raise awareness about the problem of missing data from clinical trials - prompted by an initiative by BMJ to collect evidence for House of Commons Science and Technology Select Committee. It is argued once more that industry run trials are of higher quality (a point also admitted by Goldacre).<br />
<a href="https://www.statslife.org.uk/features/1182-clinical-trial-data-publishing-but-not-as-we-know-it">Clinical trial data: it’s publishing, Jim, but not as we know it</a><br />
A piece again about the importance of publishing clinical trial data, the information generated by pharma companies and for which they are compensated by the sales. It argues however that it should be self-publication rather than in medical journals, which may for example delay the publication, reject negative studies and harm the quality of the study by requesting modifications to statistical analysis, in addition to the fact that regulators do a better job than journals. Closing remark from the article:</p>
<pre><code>We are moving from an era of private data and public analyses to one of public data and private analyses. Just as we have learned to be cautious about data that are missing, we may have to be cautious about missing analyses also.</code></pre>
<p><a href="http://andrewgelman.com/2014/03/01/moving-era-private-data-public-analyses-one-public-data-private-analyses-just-learned-cautious-data-missing-may-cautiou/">A related discussion on Andrew Gelman’s blog</a></p>
